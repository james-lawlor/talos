"""
This workflow is designed to generate a minimally annotated dataset for use in Talos
this workflow is designed to be something which could be executed easily off-site by non-CPG users

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations

* Step 7: Compress the final MatrixTable into a tarball
    - Localise the MatrixTable inside the container using gcloud
    - Compress the MatrixTable into a tarball using zstd
    - Data is written into permanent storage, and registered into Metamist
"""

from functools import cache

import loguru

from cpg_utils import Path

from cpg_flow import workflow, stage, targets, utils

from talos.cpg_internal_scripts.cpgflow_jobs import ExtractVcfFromMt, ComposeVcfFragments


SHARD_MANIFEST = 'shard-manifest.txt'


@cache
def does_final_file_path_exist(dataset: targets.Dataset) -> bool:
    """
    This workflow includes the generation of a lot of temporary files and folders
    If I run it again, I don't want to accidentally regenerate all the intermediates, even though the final output
    exists. In that scenario I just want no jobs to be planned.

    This method builds the path to the final object, and checks if it exists in GCP
    If it does, we can skip all other stages

    Args:
        dataset (Dataset):

    Returns:
        bool, whether the final file in the workflow already exists
    """
    # if the name of the SquashMtIntoTarball Stage changes, update this String
    return utils.exists(workflow.get_workflow().prefix / 'SquashMtIntoTarball' / f'{dataset.name}.mt.tar')


@stage.stage
class ExtractVcfFromDatasetMtWithHail(stage.DatasetStage):
    """
    extract some plain calls from a joint-callset
    these calls are a region-filtered subset, limited to genic regions
    """

    def expected_outputs(self, dataset: targets.Dataset) -> dict[str, Path]:

        return {
            # write path for the full (region-limited) MatrixTable, stripped of info fields
            'mt': self.tmp_prefix / f'{dataset.name}.mt',
            # this will be the write path for fragments of sites-only VCF
            'sites_only_vcf_dir': str(self.tmp_prefix / f'{dataset.name}_separate.vcf.bgz'),
            # this will be the file which contains the name of all fragments
            'sites_only_vcf_manifest': self.tmp_prefix / f'{dataset.name}_separate.vcf.bgz' / SHARD_MANIFEST,
        }

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        """
        script is called extract_vcf_from_mt
        """
        outputs = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            loguru.logger.info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, outputs, jobs=None)

        job = ExtractVcfFromMt.make_vcf_extraction_job(
            dataset=dataset,
            output_mt=outputs['mt'],
            output_sitesonly=outputs['sites_only_vcf_dir'],
            job_attrs=self.get_job_attrs(dataset)
        )

        return self.make_outputs(dataset, outputs, jobs=job)


@stage.stage(required_stages=ExtractVcfFromDatasetMtWithHail)
class ConcatenateSitesOnlyVcfFragments(stage.DatasetStage):
    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        return self.prefix / f'{dataset.name}_sites_only_reassembled.vcf.bgz'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        """
        trigger a rolling merge using gcloud compose, gluing all the individual files together
        """

        output = self.expected_outputs(dataset)

        sites_manifest = inputs.as_str(dataset, ExtractVcfFromDatasetMtWithHail, 'sites_only_vcf_manifest')

        jobs = ComposeVcfFragments.make_condense_jobs(
            dataset=dataset,
            manifest_file=sites_manifest,
            output=output,
            job_attrs=self.get_job_attrs(dataset)
        )
        return self.make_outputs(dataset, data=output, jobs=jobs)
