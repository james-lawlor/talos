"""
This workflow is designed to generate a minimally annotated dataset for use in Talos
this workflow is designed to be something which could be executed easily off-site by non-CPG users

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations

* Step 7: Compress the final MatrixTable into a tarball
    - Localise the MatrixTable inside the container using gcloud
    - Compress the MatrixTable into a tarball using zstd
    - Data is written into permanent storage, and registered into Metamist
"""

from functools import cache

import loguru

from cpg_utils import Path

from cpg_flow import workflow, stage, targets, utils

from talos.cpg_internal_scripts.cpgflow_jobs import (
    ExtractVcfFromMt,
    ComposeVcfFragments,
    AnnotateGnomadUsingEchtvar,
    AnnotateConsequenceUsingBcftools,
    SitesOnlyVcfIntoHt,
    JumpAnnotationsFromHtToFinalMt,
    SquashMtIntoTarball,
)


SHARD_MANIFEST = 'shard-manifest.txt'


@cache
def does_final_file_path_exist(dataset: targets.Dataset) -> bool:
    """
    This workflow includes the generation of a lot of temporary files and folders
    If I run it again, I don't want to accidentally regenerate all the intermediates, even though the final output
    exists. In that scenario I just want no jobs to be planned.

    This method builds the path to the final object, and checks if it exists in GCP
    If it does, we can skip all other stages

    Args:
        dataset (Dataset):

    Returns:
        bool, whether the final file in the workflow already exists
    """
    # if the name of the SquashMtIntoTarball Stage changes, update this String
    return utils.exists(workflow.get_workflow().prefix / 'SquashMtIntoTarball' / f'{dataset.name}.mt.tar')


@stage.stage
class ExtractVcfFromDatasetMtWithHail(stage.DatasetStage):
    """
    extract some plain calls from a joint-callset
    these calls are a region-filtered subset, limited to genic regions
    """

    def expected_outputs(self, dataset: targets.Dataset) -> dict[str, Path]:
        return {
            # write path for the full (region-limited) MatrixTable, stripped of info fields
            'mt': self.tmp_prefix / f'{dataset.name}.mt',
            # this will be the write path for fragments of sites-only VCF
            'sites_only_vcf_dir': str(self.tmp_prefix / f'{dataset.name}_separate.vcf.bgz'),
            # this will be the file which contains the name of all fragments
            'sites_only_vcf_manifest': self.tmp_prefix / f'{dataset.name}_separate.vcf.bgz' / SHARD_MANIFEST,
        }

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        """
        script is called extract_vcf_from_mt
        """
        outputs = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            loguru.logger.info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, outputs, jobs=None)

        job = ExtractVcfFromMt.make_vcf_extraction_job(
            dataset=dataset,
            output_mt=outputs['mt'],
            output_sitesonly=outputs['sites_only_vcf_dir'],
            job_attrs=self.get_job_attrs(dataset),
        )

        return self.make_outputs(dataset, outputs, jobs=job)


@stage.stage(required_stages=ExtractVcfFromDatasetMtWithHail)
class ConcatenateSitesOnlyVcfFragments(stage.DatasetStage):
    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        return self.prefix / f'{dataset.name}_sites_only_reassembled.vcf.bgz'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        """
        trigger a rolling merge using gcloud compose, gluing all the individual files together
        """

        output = self.expected_outputs(dataset)

        sites_manifest = inputs.as_str(dataset, ExtractVcfFromDatasetMtWithHail, 'sites_only_vcf_manifest')
        manifest_dir = inputs.as_str(dataset, ExtractVcfFromDatasetMtWithHail, 'sites_only_vcf_dir')

        jobs = ComposeVcfFragments.make_condense_jobs(
            dataset=dataset,
            manifest_file=sites_manifest,
            manifest_dir=manifest_dir,
            output=output,
            tmp_dir=self.tmp_prefix,
            job_attrs=self.get_job_attrs(dataset),
        )
        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage.stage(required_stages=ConcatenateSitesOnlyVcfFragments)
class AnnotateGnomadUsingEchtvarStage(stage.DatasetStage):
    """
    Annotate this cohort joint-call VCF with gnomad frequencies, write to tmp storage
    """

    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}_gnomad_frequency_annotated.vcf.bgz'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(dataset)

        site_only_vcf = inputs.as_str(dataset, ConcatenateSitesOnlyVcfFragments)

        if does_final_file_path_exist(dataset):
            loguru.logger.info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        job = AnnotateGnomadUsingEchtvar.make_echtvar_job(
            dataset=dataset,
            sites_only_vcf=site_only_vcf,
            output=output,
            job_attrs=self.get_job_attrs(dataset),
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage.stage(required_stages=AnnotateGnomadUsingEchtvarStage)
class AnnotateConsequenceUsingBcftoolsStage(stage.DatasetStage):
    """
    Take the VCF with gnomad frequencies, and annotate with consequences using BCFtools
    Writes into a cohort-specific permanent folder
    """

    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}_consequence_annotated.vcf.bgz'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            loguru.logger.info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        gnomad_annotated_vcf = inputs.as_path(dataset, AnnotateGnomadUsingEchtvarStage)

        job = AnnotateConsequenceUsingBcftools.make_bcftools_anno_job(
            dataset=dataset,
            gnomad_vcf=gnomad_annotated_vcf,
            output=output,
            job_attrs=self.get_job_attrs(dataset),
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage.stage(required_stages=AnnotateConsequenceUsingBcftoolsStage)
class SitesOnlyVcfIntoAnnotationsHt(stage.DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        # output will be a tarball, containing the {dataset.name}_annotations.ht directory
        return self.tmp_prefix / f'{dataset.name}_annotations.ht'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            loguru.logger.info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        bcftools_vcf = inputs.as_str(dataset, AnnotateConsequenceUsingBcftoolsStage)

        job = SitesOnlyVcfIntoHt.make_vcf_to_ht_job(
            dataset=dataset,
            bcftools_vcf=bcftools_vcf,
            output_ht=output,
            tmp_dir=self.tmp_prefix / f'{dataset.name}_annotation_checkpoint',
            job_attrs=self.get_job_attrs(dataset),
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage.stage(required_stages=[SitesOnlyVcfIntoAnnotationsHt, ExtractVcfFromDatasetMtWithHail])
class JumpAnnotationsFromHtToFinalMtStage(stage.DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}.mt'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            loguru.logger.info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        # get the region-limited MT
        mt = inputs.as_str(dataset, ExtractVcfFromDatasetMtWithHail, 'mt')

        # get the table of compressed annotations
        annotations = inputs.as_str(dataset, SitesOnlyVcfIntoAnnotationsHt)

        job = JumpAnnotationsFromHtToFinalMt.make_vcf_to_ht_job(
            dataset=dataset,
            annotations_ht=annotations,
            input_mt=mt,
            output_mt=output,
            job_attrs=self.get_job_attrs(dataset),
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage.stage(
    analysis_type='talos_prep',
    required_stages=[JumpAnnotationsFromHtToFinalMtStage],
)
class SquashMtIntoTarballStage(stage.DatasetStage):
    """
    Localise the MatrixTable, and create a tarball
    Don't attempt additional compression - it's
    Means that Talos downstream of this won't need GCloud installed
    """

    def expected_outputs(self, dataset: targets.Dataset) -> Path:
        return self.prefix / f'{dataset.name}.mt.tar'

    def queue_jobs(self, dataset: targets.Dataset, inputs: stage.StageInput) -> stage.StageOutput:
        output = self.expected_outputs(dataset)

        input_mt = inputs.as_str(dataset, JumpAnnotationsFromHtToFinalMtStage)

        job = SquashMtIntoTarball.make_tarball_squash_job(
            dataset=dataset,
            input_mt=input_mt,
            output_tar=output,
            job_attrs=self.get_job_attrs(dataset),
        )

        return self.make_outputs(dataset, data=output, jobs=job)
